---
class: colored-edge colored-edge-alis
---

# Stable Diffusion 1.5

- Allows for image-to-image generations
- Finer-grain control over generated output
- Runs on consumer hardware
<!--
- Can make AI-powered edits to images (AKA "inpainting")
-->

Technical details:
- GPU: Nvidia GeForce RTX 3080 Ti (12 GB VRAM)
- Browser UI: [Automatic1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui)
- Base model: [Anything V3](https://civitai.com/models/66/anything-v3) (SD 1.5 fine-tune for anime-style art)

[stable-diffusion-technical-primer]: https://redd.it/13belgg/

<!--
- I switched to Stable Diffusion because I could run it locally instead of needing to buy credits for images I'm gambling on getting right.
- Also, as it turns out, customizable workflows are pretty nice too for controlling your output.
-->

<!--
The next image model I used was Stable Diffusion 1.5.

Like DALL-E 2, Stable Diffusion could generate images from images. Instead of how DALL-E 2 generated image variations, though, Stable Diffusion would let you generate an image on top of an existing image.
-->
